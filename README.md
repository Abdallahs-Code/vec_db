# VecDB â€“ Disk-Based Vector Index for Semantic Search

## Overview

VecDB is a lightweight, disk-backed vector database and indexing system implemented in pure Python. It is designed for large-scale semantic search over high-dimensional embeddings (millions of vectors) under strict memory constraints. The system builds an **approximate nearest neighbor (ANN)** index using **Inverted File (IVF) clustering** combined with **Product Quantization (PQ)** and supports efficient top-k retrieval using cosine similarity.

## Key Features

* Disk-based storage using memory-mapped files
* IVF index with MiniBatch K-Means clustering
* Product Quantization (PQ) for compressed vector storage
* Asymmetric distance computation (ADC) for fast retrieval
* Scales up to Millions of vectors under tight RAM and time limits
* Pure Python + NumPy + scikit-learn

## Architecture

1. **Vector Storage**: Raw vectors are stored sequentially on disk as float32.
2. **Clustering (IVF)**: Vectors are assigned to centroids learned via MiniBatchKMeans.
3. **Compression (PQ)**: Each vector is encoded into compact PQ codes to reduce index size.
4. **Inverted Lists**: For each cluster, vector IDs and PQ codes are stored on disk.
5. **Retrieval**:

   * Select top `nprobe` clusters using cosine similarity to centroids
   * Compute approximate distances using PQ lookup tables
   * Re-rank candidates using exact cosine similarity

## Constraints and Design Goals

* No in-memory caching during retrieval
* All index structures persisted on disk
* Efficient under strict RAM and latency limits
* Approximate search with high recall

## Semantic Search in Modern AI Systems

Semantic search is a core component of many modern AI systems. Instead of relying on exact keyword matching, semantic search represents data (text, images, audio, etc.) as dense numerical vectors generated by machine learning models. These vectors capture meaning and context, enabling systems to retrieve results that are conceptually similar rather than lexically identical.

In AI applications, semantic search is commonly used as the retrieval layer in **Retrieval-Augmented Generation (RAG)** architectures. In a RAG system, an embedding model converts user queries into vectors, which are then compared against a large vector index to retrieve the most relevant items. The retrieved results are supplied as external knowledge to a large language model (LLM), improving factual accuracy, grounding responses in real data, and reducing hallucinations.

This approach is widely used in question answering systems, document search, recommendation engines, and multimodal search (such as image or video retrieval). Efficient vector indexing and similarity search are therefore essential building blocks for scalable and practical AI systems.

## Note

This project does **not** rely on external vector databases or libraries such as FAISS or HNSW.